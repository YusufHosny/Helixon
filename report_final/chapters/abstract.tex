Head-Tracking systems play a crucial role in various fields such as augmented reality, gaming, and immersive audio. It is therefore essential for these systems to be as accurate as possible, in order to provide immersive experiences for the users. Specifically in the context of spatial audio, knowledge of the positioning of the head allows the alignment of auditory experiences with visual stimuli. One example of that would be in VR environments, where sound sources must correspond to visual cues to create a realistic 3D audio experience. The current problem with most of the state of the art solutions, is that they are often complex, expensive, and computationally intensive. This ends up limiting their accessability.  This project aims to tackle these problems by combining several signal processing algorithms to enhance a cost-effective, head-tracking system. We explore multiple approaches to estimating position and orientation, and we present a collected and labelled dataset of 110 minutes of motion. Our method acheieves average error of 2m and 35 degrees on our collected dataset.